{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing useful libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import os\n",
    "#print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_12</th>\n",
       "      <th>Feature_13</th>\n",
       "      <th>Feature_14</th>\n",
       "      <th>Feature_15</th>\n",
       "      <th>Feature_16</th>\n",
       "      <th>Feature_17</th>\n",
       "      <th>Feature_18</th>\n",
       "      <th>Feature_19</th>\n",
       "      <th>Feature_20</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-90.532634</td>\n",
       "      <td>-66.953472</td>\n",
       "      <td>79.261856</td>\n",
       "      <td>-111.800554</td>\n",
       "      <td>126.403549</td>\n",
       "      <td>111.509348</td>\n",
       "      <td>207.728953</td>\n",
       "      <td>-26.600556</td>\n",
       "      <td>-106.229324</td>\n",
       "      <td>...</td>\n",
       "      <td>148.391734</td>\n",
       "      <td>77.835738</td>\n",
       "      <td>-10.728207</td>\n",
       "      <td>1.100625</td>\n",
       "      <td>1.035362</td>\n",
       "      <td>-87.010809</td>\n",
       "      <td>-16.242297</td>\n",
       "      <td>-32.074925</td>\n",
       "      <td>-17.666026</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-103.156720</td>\n",
       "      <td>-16.070400</td>\n",
       "      <td>87.819228</td>\n",
       "      <td>12.614599</td>\n",
       "      <td>51.347780</td>\n",
       "      <td>67.483725</td>\n",
       "      <td>40.269172</td>\n",
       "      <td>51.442254</td>\n",
       "      <td>-151.486693</td>\n",
       "      <td>...</td>\n",
       "      <td>29.009475</td>\n",
       "      <td>3.995786</td>\n",
       "      <td>-10.861630</td>\n",
       "      <td>-142.605726</td>\n",
       "      <td>-25.924592</td>\n",
       "      <td>-86.755351</td>\n",
       "      <td>-36.479749</td>\n",
       "      <td>-130.246619</td>\n",
       "      <td>-44.143652</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>24.326153</td>\n",
       "      <td>-92.098078</td>\n",
       "      <td>82.238354</td>\n",
       "      <td>-56.795879</td>\n",
       "      <td>85.203996</td>\n",
       "      <td>127.916504</td>\n",
       "      <td>-90.080307</td>\n",
       "      <td>-128.124071</td>\n",
       "      <td>18.036020</td>\n",
       "      <td>...</td>\n",
       "      <td>111.810098</td>\n",
       "      <td>65.826018</td>\n",
       "      <td>-101.271203</td>\n",
       "      <td>-44.127749</td>\n",
       "      <td>-7.131464</td>\n",
       "      <td>-105.049759</td>\n",
       "      <td>-130.948256</td>\n",
       "      <td>-43.113523</td>\n",
       "      <td>-37.330448</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-64.631737</td>\n",
       "      <td>-83.703583</td>\n",
       "      <td>84.135072</td>\n",
       "      <td>-5.516152</td>\n",
       "      <td>74.338494</td>\n",
       "      <td>112.630556</td>\n",
       "      <td>181.576798</td>\n",
       "      <td>-1.054023</td>\n",
       "      <td>60.469865</td>\n",
       "      <td>...</td>\n",
       "      <td>50.047108</td>\n",
       "      <td>100.439101</td>\n",
       "      <td>-117.842955</td>\n",
       "      <td>150.239788</td>\n",
       "      <td>-144.635542</td>\n",
       "      <td>-144.306209</td>\n",
       "      <td>-69.272905</td>\n",
       "      <td>-79.629675</td>\n",
       "      <td>-51.334456</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-55.473830</td>\n",
       "      <td>-78.853237</td>\n",
       "      <td>88.129107</td>\n",
       "      <td>75.200543</td>\n",
       "      <td>76.991520</td>\n",
       "      <td>60.224711</td>\n",
       "      <td>-13.106559</td>\n",
       "      <td>-146.773016</td>\n",
       "      <td>-33.490566</td>\n",
       "      <td>...</td>\n",
       "      <td>85.988282</td>\n",
       "      <td>23.381960</td>\n",
       "      <td>11.876102</td>\n",
       "      <td>-188.296503</td>\n",
       "      <td>-80.323929</td>\n",
       "      <td>-56.757987</td>\n",
       "      <td>-20.314172</td>\n",
       "      <td>-42.625170</td>\n",
       "      <td>-24.102753</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Feature_1  Feature_2  Feature_3   Feature_4   Feature_5   Feature_6  \\\n",
       "0   1  -90.532634 -66.953472  79.261856 -111.800554  126.403549  111.509348   \n",
       "1   2 -103.156720 -16.070400  87.819228   12.614599   51.347780   67.483725   \n",
       "2   3   24.326153 -92.098078  82.238354  -56.795879   85.203996  127.916504   \n",
       "3   4  -64.631737 -83.703583  84.135072   -5.516152   74.338494  112.630556   \n",
       "4   5  -55.473830 -78.853237  88.129107   75.200543   76.991520   60.224711   \n",
       "\n",
       "    Feature_7   Feature_8   Feature_9  ...  Feature_12  Feature_13  \\\n",
       "0  207.728953  -26.600556 -106.229324  ...  148.391734   77.835738   \n",
       "1   40.269172   51.442254 -151.486693  ...   29.009475    3.995786   \n",
       "2  -90.080307 -128.124071   18.036020  ...  111.810098   65.826018   \n",
       "3  181.576798   -1.054023   60.469865  ...   50.047108  100.439101   \n",
       "4  -13.106559 -146.773016  -33.490566  ...   85.988282   23.381960   \n",
       "\n",
       "   Feature_14  Feature_15  Feature_16  Feature_17  Feature_18  Feature_19  \\\n",
       "0  -10.728207    1.100625    1.035362  -87.010809  -16.242297  -32.074925   \n",
       "1  -10.861630 -142.605726  -25.924592  -86.755351  -36.479749 -130.246619   \n",
       "2 -101.271203  -44.127749   -7.131464 -105.049759 -130.948256  -43.113523   \n",
       "3 -117.842955  150.239788 -144.635542 -144.306209  -69.272905  -79.629675   \n",
       "4   11.876102 -188.296503  -80.323929  -56.757987  -20.314172  -42.625170   \n",
       "\n",
       "   Feature_20  Class  \n",
       "0  -17.666026      1  \n",
       "1  -44.143652      2  \n",
       "2  -37.330448      4  \n",
       "3  -51.334456      2  \n",
       "4  -24.102753      2  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('~/Downloads/multi_classification_train(1).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting and normalising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature_1  Feature_2  Feature_3   Feature_4   Feature_5   Feature_6  \\\n",
      "0  -90.532634 -66.953472  79.261856 -111.800554  126.403549  111.509348   \n",
      "1 -103.156720 -16.070400  87.819228   12.614599   51.347780   67.483725   \n",
      "2   24.326153 -92.098078  82.238354  -56.795879   85.203996  127.916504   \n",
      "3  -64.631737 -83.703583  84.135072   -5.516152   74.338494  112.630556   \n",
      "4  -55.473830 -78.853237  88.129107   75.200543   76.991520   60.224711   \n",
      "\n",
      "    Feature_7   Feature_8   Feature_9  Feature_10  Feature_11  Feature_12  \\\n",
      "0  207.728953  -26.600556 -106.229324  170.384649   69.038320  148.391734   \n",
      "1   40.269172   51.442254 -151.486693   35.872507   28.016774   29.009475   \n",
      "2  -90.080307 -128.124071   18.036020   70.485076   56.468318  111.810098   \n",
      "3  181.576798   -1.054023   60.469865   -0.658352   35.245623   50.047108   \n",
      "4  -13.106559 -146.773016  -33.490566  137.684666   47.595552   85.988282   \n",
      "\n",
      "   Feature_13  Feature_14  Feature_15  Feature_16  Feature_17  Feature_18  \\\n",
      "0   77.835738  -10.728207    1.100625    1.035362  -87.010809  -16.242297   \n",
      "1    3.995786  -10.861630 -142.605726  -25.924592  -86.755351  -36.479749   \n",
      "2   65.826018 -101.271203  -44.127749   -7.131464 -105.049759 -130.948256   \n",
      "3  100.439101 -117.842955  150.239788 -144.635542 -144.306209  -69.272905   \n",
      "4   23.381960   11.876102 -188.296503  -80.323929  -56.757987  -20.314172   \n",
      "\n",
      "   Feature_19  Feature_20  \n",
      "0  -32.074925  -17.666026  \n",
      "1 -130.246619  -44.143652  \n",
      "2  -43.113523  -37.330448  \n",
      "3  -79.629675  -51.334456  \n",
      "4  -42.625170  -24.102753  \n",
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
      "0  -0.565763  -0.572228  -2.150885  -1.130486   2.109573   0.699686   \n",
      "1  -0.920118   0.218842   0.544938  -0.302313  -1.490166  -0.194482   \n",
      "2   2.658295  -0.963146  -1.213201  -0.764346   0.133607   1.032917   \n",
      "3   0.161269  -0.832639  -0.615679  -0.423001  -0.387511   0.722457   \n",
      "4   0.418329  -0.757231   0.642559   0.114292  -0.260270  -0.341913   \n",
      "\n",
      "   Feature_7  Feature_8  Feature_9  Feature_10  Feature_11  Feature_12  \\\n",
      "0   2.245811  -0.482585  -0.453178    1.452740    1.437620    1.437620   \n",
      "1   0.241472   0.260107  -0.890384    0.442213   -0.322273   -0.322273   \n",
      "2  -1.318691  -1.448732   0.747282    0.702241    0.898346    0.898346   \n",
      "3   1.932794  -0.239472   1.157212    0.167773   -0.012143   -0.012143   \n",
      "4  -0.397386  -1.626204   0.249512    1.207080    0.517689    0.517689   \n",
      "\n",
      "   Feature_13  Feature_14  Feature_15  Feature_16  Feature_17  Feature_18  \\\n",
      "0    0.128782    0.505573    0.649017   -0.164009   -0.555821    0.676735   \n",
      "1   -1.646789    0.504300   -0.658786   -0.463134   -0.551493   -0.034147   \n",
      "2   -0.160006   -0.357950    0.237415   -0.254621   -0.861421   -3.352544   \n",
      "3    0.672308   -0.515997    2.006262   -1.780250   -1.526470   -1.186072   \n",
      "4   -1.180625    0.721154   -1.074596   -1.066703   -0.043304    0.533702   \n",
      "\n",
      "   Feature_19  Feature_20  \n",
      "0    1.730322    1.452740  \n",
      "1   -0.987663    0.442213  \n",
      "2    1.424707    0.702241  \n",
      "3    0.413720    0.167773  \n",
      "4    1.438228    1.207080  \n",
      "(48000, 20)\n",
      "(48000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = data.iloc[:,1:21]\n",
    "print(x.head())\n",
    "x = (x - x.mean())/x.std()\n",
    "print(x.head())\n",
    "x = x.values\n",
    "print(x.shape)\n",
    "y = data.iloc[:,21].values\n",
    "print(y.shape)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining one hot encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .values se ek column wala numpy array nhi bnta\n",
    "def one_hot_encode(arr):\n",
    "    \n",
    "\n",
    "    unique_values = np.unique(arr) # gibr a list of unique elements in arr\n",
    "    \n",
    "   \n",
    "    value_to_index = {val: idx for idx, val in enumerate(unique_values)} #  mapping of unique values to indices\n",
    "    \n",
    "  \n",
    "    one_hot_matrix = np.zeros((len(arr), len(unique_values)), dtype = np.int64)        # Initialize a zero matrix of shape (len(arr), len(unique_values))\n",
    "    \n",
    "    \n",
    "    for i, val in enumerate(arr):\n",
    "        one_hot_matrix[i, value_to_index[val]] = 1    # Setting the appropriate indices to 1\n",
    "    \n",
    "    return one_hot_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spilliting data for cross validation and taking transpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "(20, 38400)\n",
      "(9600,)\n",
      "[1 2 4 ... 3 4 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print((y.shape))\n",
    "m = x.shape[0]\n",
    "\n",
    "n = int(0.2*m)\n",
    "x_test = x[:n,:].T\n",
    "x_train = x[n:,:].T\n",
    "print(x_train.shape)\n",
    "\n",
    "y_test = y[:n].T\n",
    "y_train_not_one = y[n:]\n",
    "print(y_test.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding of training output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 ... 1 3 2]\n",
      "[0 1 2 3 4]\n",
      "(38400,)\n",
      "2    0.346208\n",
      "1    0.237583\n",
      "3    0.209667\n",
      "4    0.122375\n",
      "0    0.084167\n",
      "Name: proportion, dtype: float64\n",
      "(5, 38400)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [1 0 1 ... 0 0 1]\n",
      " [0 1 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(5, 38400)\n",
      "[1 2 4 ... 3 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_not_one)\n",
    "a = np.unique(y_train_not_one)\n",
    "print(a)\n",
    "print(y_train_not_one.shape)\n",
    "class_counts = pd.Series(y).value_counts(normalize = True)  # normalize = true krne pr proprtion deta hai \n",
    "print(class_counts) \n",
    "y_train = one_hot_encode(y_train_not_one).T\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(a3,y,l,w1,w2,w3):\n",
    "    \n",
    "    e = 1e-10  # for safety \n",
    "    m = y.shape[1]\n",
    "    cost  = -(1/m)*(np.sum(y*np.log(a3 + e)))+ (l/(2*m))*(np.sum(np.square(w1))+ np.sum(np.square(w2)) + np.sum(np.square(w3)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_x = x_train.shape[0]\n",
    "n_h1 = 1024\n",
    "n_h2 = 512\n",
    "n_y = y_train.shape[0]\n",
    "alpha = 0.01\n",
    "iteration = 100\n",
    "l = 0.0001\n",
    "\n",
    "w1 = np.random.randn(n_h1,n_x)*np.sqrt(2/n_x)\n",
    "\n",
    "b1 = np.zeros((n_h1, 1))\n",
    "\n",
    "w2 = np.random.randn(n_h2,n_h1)*np.sqrt(2/n_h1)\n",
    "\n",
    "b2 = np.zeros((n_h2,1))\n",
    "\n",
    "w3 = np.random.randn(n_y,n_h2)*0.01\n",
    "\n",
    "b3 = np.zeros((n_y,1))\n",
    "\n",
    "def relu(x_train):\n",
    "    return np.maximum(x_train, 0)\n",
    "def softmax(x_train):\n",
    "    expX = np.exp(x_train)\n",
    "    return expX/np.sum(expX, axis=0)\n",
    "    \n",
    "\n",
    "def d_relu(x_train):\n",
    "    return np.array(x_train > 0, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1500,1024) and (80000,1024) not aligned: 1024 (dim 1) != 80000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m j_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iteration):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     20\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m relu(z1)\n\u001b[1;32m     21\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(w2, a1) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1500,1024) and (80000,1024) not aligned: 1024 (dim 1) != 80000 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_accuracy(a3,y):\n",
    "    a3_index = np.argmax(a3, axis = 0) \n",
    "    accuracy = (np.mean(a3_index == y))*100\n",
    "    return accuracy\n",
    "\n",
    "# Initialize Adam parameters\n",
    "beta1 = 0.9  # Exponential decay for the first moment estimates\n",
    "beta2 = 0.999  # Exponential decay for the second moment estimates\n",
    "epsilon = 1e-8  # Small constant to avoid division by zero\n",
    "alpha_initial = 0.001  # Learning rate\n",
    "decay_rate = 0.96\n",
    "decay_step = 100\n",
    "l = 0.01\n",
    "# Initialize moment estimates for all parameters\n",
    "m_w1, m_w2, m_w3 = 0, 0, 0  # First moments for weights\n",
    "v_w1, v_w2, v_w3 = 0, 0, 0  # Second moments for weights\n",
    "m_b1, m_b2, m_b3 = 0, 0, 0  # First moments for biases\n",
    "v_b1, v_b2, v_b3 = 0, 0, 0  # Second moments for biases\n",
    "\n",
    "t = 0  # Time step for bias correction\n",
    "\n",
    "j_history = []\n",
    "for i in range(iteration):\n",
    "    # Forward propagation\n",
    "    z1 = np.dot(w1, x) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(w3, a2) + b3\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    # Backward propagation\n",
    "    m = x.shape[1]\n",
    "    dz3 = (a3 - y_train)\n",
    "    dw3 = (1/m) * (np.dot(dz3, a2.T) + l * np.sum(w3))\n",
    "    db3 = (1/m) * np.sum(dz3, axis=1, keepdims=True)\n",
    "    dz2 = (1/m) * np.dot(w3.T, dz3) * d_relu(a2)\n",
    "    dw2 = (1/m) * (np.dot(dz2, a1.T) + l * np.sum(w2))\n",
    "    db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "    dz1 = (1/m) * np.dot(w2.T, dz2) * d_relu(a1)\n",
    "    dw1 = (1/m) * (np.dot(dz1, x.T) + l * np.sum(w1))\n",
    "    db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "    \n",
    "    # Update Adam parameters\n",
    "    t += 1\n",
    "    alpha = alpha_initial * (decay_rate ** (t // decay_step))\n",
    "    # Update weights and biases using Adam\n",
    "    for param, grad, m, v in zip(\n",
    "        [w1, w2, w3, b1, b2, b3],\n",
    "        [dw1, dw2, dw3, db1, db2, db3],\n",
    "        [m_w1, m_w2, m_w3, m_b1, m_b2, m_b3],\n",
    "        [v_w1, v_w2, v_w3, v_b1, v_b2, v_b3],\n",
    "    ):\n",
    "        m = beta1 * m + (1 - beta1) * grad  # Update biased first moment\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)  # Update biased second moment\n",
    "        m_corrected = m / (1 - beta1 ** t)  # Correct bias for first moment\n",
    "        v_corrected = v / (1 - beta2 ** t)  # Correct bias for second moment\n",
    "        param -= alpha * m_corrected / (np.sqrt(v_corrected) + epsilon)  # Update parameter\n",
    "    \n",
    "    # Compute and store cost\n",
    "    if i < iteration:\n",
    "        c = cost_function(a3, y_train, l, w1, w2, w3)\n",
    "        j_history.append(c)\n",
    "        \n",
    "    \n",
    "    # Print cost at intervals\n",
    "    if (i%(iteration/10) == 0):\n",
    "        accuracy = compute_accuracy(a3, y_train_not_one)\n",
    "        print(f'Iteration {i+1}, Cost: {c}, Accuracy: {accuracy}')\n",
    "\n",
    "# Plot cost history\n",
    "plt.plot(j_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost Reduction Over Iterations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 9600)\n",
      "[1 2 4 ... 3 3 2]\n",
      "2    0.353854\n",
      "1    0.241354\n",
      "3    0.210729\n",
      "4    0.118438\n",
      "0    0.075625\n",
      "Name: proportion, dtype: float64\n",
      "accuracy of cross validation set is 96.1875\n",
      "(5, 38400)\n",
      "[2 3 2 ... 1 3 2]\n",
      "2    0.350859\n",
      "1    0.237969\n",
      "3    0.211068\n",
      "4    0.120938\n",
      "0    0.079167\n",
      "Name: proportion, dtype: float64\n",
      "[2 3 2 ... 1 3 2]\n",
      "[2 3 2 ... 1 3 2]\n",
      "(38400,)\n",
      "accuracy of training set is 96.64583333333333\n"
     ]
    }
   ],
   "source": [
    "def prediction(x_test,w1,w2,w3,b1,b2,b3):\n",
    "    z1 = np.dot(w1,x_test) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(w2,a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(w3,a2) + b3\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "\n",
    "    print((a3.shape))\n",
    "\n",
    "    a3_index = np.argmax(a3, axis = 0) \n",
    "    \n",
    "    print(a3_index)\n",
    "    return a3_index\n",
    "a3_index = prediction(x_test,w1,w2,w3,b1,b2,b3)\n",
    "\n",
    "\n",
    "class_counts = pd.Series(a3_index).value_counts(normalize = True)  # normalize = true krne pr proprtion deta hai \n",
    "print(class_counts) \n",
    "accuracy = np.mean(a3_index == y_test)\n",
    "print(f\"accuracy of cross validation set is {accuracy*100}\")\n",
    "\n",
    "a3_train_index = prediction(x_train,w1,w2,w3,b1,b2,b3)\n",
    "class_counts = pd.Series(a3_train_index).value_counts(normalize = True)  \n",
    "print(class_counts) \n",
    "print(a3_train_index)\n",
    "print(y_train_not_one)\n",
    "print(y_train_not_one.shape)\n",
    "print(f\"accuracy of training set is {np.mean(a3_train_index == y_train_not_one)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.9623253064306274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def f1_score_avg_weighted(y, y_test, n_classes):\n",
    "    f1_scores = np.zeros(n_classes)\n",
    "    class_count = np.zeros(n_classes)  \n",
    "    e = 1e-9  \n",
    "    \n",
    "    for i in range(n_classes):  \n",
    "        precision = np.sum((y == i) & (y_test == i)) / (np.sum(y == i) + e)\n",
    "        recall = np.sum((y == i) & (y_test == i)) / (np.sum(y_test == i) + e)\n",
    "        f1_scores[i] = 2 * (precision * recall) / (precision + recall + e)\n",
    "        class_count[i] = np.sum((y == i) & (y_test == i))\n",
    "    \n",
    "    return (np.sum(f1_scores * class_count)/(np.sum(class_count) + e)) \n",
    "\n",
    "\n",
    "n_classes = len(np.unique(y_test))  \n",
    "f1_avg = f1_score_avg_weighted(a3_index, y_test, n_classes)\n",
    "print(f\"Average F1 Score: {f1_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('~/Downloads/multi_classification_test.csv')\n",
    "x_test = data_test.iloc[:,1:]\n",
    "x_test = (x_test - x_test.mean())/(x_test.std())\n",
    "x_test = x_test.values\n",
    "y_pred = prediction(x_test,w1,w2,w3,b1,b2,b3) \n",
    "print(y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
